# -*- coding: utf-8 -*-
"""train-phi-1.5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VpBUUi-oFu7bfDmuIRVLQCS-pk4WaGx5
"""

#!pip install accelerate transformers einops datasets peft bitsandbytes --upgrade

from peft import PeftModel
from transformers import AutoModelForCausalLM
import torch
from datasets import load_dataset, Dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model
import os

model_name = "susnato/phi-1_5_dev"
#model_name = "microsoft/phi-1"

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

tokenizer.pad_token = tokenizer.eos_token

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)

bnb_config_8 = BitsAndBytesConfig(
    load_in_8bit=True,
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map={"":0},
    trust_remote_code=True,
    torch_dtype="auto",
    quantization_config=bnb_config
)

print(model)

#from bitsandbytes.nn.modules import Linear8bitLt, Linear4bit
#import torch.nn.init as init  # Importing the init module from PyTorch
#import bitsandbytes as bnb
#from contextlib import contextmanager

#def noop (x=None, *args, **kwargs):
#    "Do nothing"
#    return x

#@contextmanager
#def no_kaiming():
#    old_iku = init.kaiming_uniform_
#    init.kaiming_uniform_ = noop
#    try: yield
#    finally: init.kaiming_uniform_ = old_iku

#_old_8init = Linear8bitLt.__init__
#_old_4init = Linear4bit.__init__

#def _new_4init(self, input_features, output_features, bias=True,
#               device=None, **kwargs):
#    with no_kaiming():
#        return _old_4init(self, input_features, output_features, bias=bias,
#                          device=device, **kwargs)



#def _new_8init(self, input_features, output_features, bias=True, has_fp16_weights=True,
#              memory_efficient_backward=False, threshold=0.0, index=None, device=None):
#    with no_kaiming():
#        return _old_8init(self, input_features, output_features, bias=bias, has_fp16_weights=has_fp16_weights,
#                          memory_efficient_backward=memory_efficient_backward, threshold=threshold, index=index, device=device)

#Linear8bitLt.__init__ = _new_8init
#Linear4bit.__init__ = _new_4init

#model

lora_config = LoraConfig(
    r=16,
    lora_alpha=16,
    #target_modules=["Wqkv", "out_proj"],
    target_modules=["query_key_value","dense","fc1","fc2"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)

model.print_trainable_parameters()

def tokenize(sample):
    tokenized_text =  tokenizer(sample["text"], padding=True, truncation=True, max_length=1024)
    return tokenized_text

#data = load_dataset("BI55/MedText", "main", split="train")
data = load_dataset("BI55/MedText", "default", split="train")

data_df = data.to_pandas()

data_df["text"] = data_df[["Prompt", "Completion"]].apply(lambda x: "Prompt: " + x["Prompt"] + " Completion: " + x["Completion"], axis=1)

data = Dataset.from_pandas(data_df)

tokenized_data = data.map(tokenize, batched=True, desc="Tokenizing data", remove_columns=data.column_names)

tokenized_data

training_arguments = TrainingArguments(
        output_dir="phi-1_5-finetuned-med-text",
        per_device_train_batch_size=14,
        gradient_accumulation_steps=1,
        learning_rate=2e-4,
        lr_scheduler_type="cosine",
        save_strategy="epoch",
        logging_steps=100,
        max_steps=1000,
        num_train_epochs=1,
        overwrite_output_dir=True
    )

trainer = Trainer(
    model=model,
    train_dataset=tokenized_data,
    args=training_arguments,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)
)
trainer.train()

"""#Save fine-tuned model"""

model.save_pretrained("phi-1_5-finetuned-med-text")

model = AutoModelForCausalLM.from_pretrained("microsoft/phi-1_5", trust_remote_code=True, torch_dtype=torch.float32)

peft_model = PeftModel.from_pretrained(model, "phi-1_5-finetuned-med-text", from_transformers=True)

model = peft_model.merge_and_unload()

# model

model.save_pretrained("phi-1_5-finetuned-med-text")

"""#Inference"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

#model = AutoModelForCausalLM.from_pretrained("phi-1_5-finetuned-med-text", trust_remote_code=True, torch_dtype=torch.float32)
model = AutoModelForCausalLM.from_pretrained("phi-1_5-finetuned-med-text", trust_remote_code=True, torch_dtype=torch.float16, device_map={"":0})

tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-1_5", trust_remote_code=True)

inputs = tokenizer('I am having a headache with but no fever - what could be the cause', return_tensors="pt", return_attention_mask=False).to(model.device)

outputs = model.generate(**inputs, max_length=2048, eos_token_id=tokenizer.eos_token_id).to(torch.device('cpu'))

text = tokenizer.batch_decode(outputs)[0]

print(text)

